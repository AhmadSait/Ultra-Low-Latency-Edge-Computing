{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97084e72-ec31-46c4-bba6-99999cb8a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max's original validation code\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "# Load CIFAR-100 test dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load the model\n",
    "model_name = \"jialicheng/cifar100-resnet-50\"\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)  # Move model to GPU if available\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test_model_performance(model, test_loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "    inference_times = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "\n",
    "            start_time = time.time()\n",
    "            outputs = model(images).logits\n",
    "            end_time = time.time()\n",
    "\n",
    "            inference_times.append(end_time - start_time)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.2f}%, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return accuracy, avg_loss, avg_inference_time\n",
    "\n",
    "def get_model_size(model):\n",
    "    temp_path = \"temp.pth\"\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    size_in_mb = os.path.getsize(temp_path) / (1024 * 1024)\n",
    "    os.remove(temp_path)  # Clean up the temporary file\n",
    "    return size_in_mb\n",
    "\n",
    "#def save_model(model, save_path):\n",
    "    #\"\"\"Save the model's state dictionary to the specified path.\"\"\"\n",
    "    #torch.save(model.state_dict(), save_path)\n",
    "    #print(f'Model saved to {save_path}')\n",
    "\n",
    "print(\"Testing the compressed model...\")\n",
    "accuracy, avg_loss, avg_inference_time = test_model_performance(model, test_loader)\n",
    "\n",
    "model_size = get_model_size(model)\n",
    "print(f'Model Size: {model_size:.2f} MB')\n",
    "\n",
    "# Example baseline values (replace with your actual baseline results)\n",
    "baseline_accuracy = 75.00\n",
    "baseline_inference_time = 0.08\n",
    "baseline_model_size = 100.0\n",
    "\n",
    "accuracy_drop = baseline_accuracy - accuracy\n",
    "speed_improvement = (baseline_inference_time - avg_inference_time) / baseline_inference_time * 100\n",
    "size_reduction = (baseline_model_size - model_size) / baseline_model_size * 100\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f'Accuracy Drop: {accuracy_drop:.2f}%')\n",
    "print(f'Speed Improvement: {speed_improvement:.2f}%')\n",
    "print(f'Size Reduction: {size_reduction:.2f}%')\n",
    "\n",
    "# Save the model after testing\n",
    "#save_path = \"cifar100_resnet50_model.pth\" \n",
    "#save_model(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569effbe-1946-4a6d-9387-76a954dec6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "# ====================================================\n",
    "# Dataset Preparation: CIFAR-100\n",
    "# ====================================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR100(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR100(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7949a75-49eb-464a-a45b-6d4862d51194",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, temperature=4.0, alpha=0.5):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # Soft loss\n",
    "        soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "            torch.log_softmax(student_logits / self.temperature, dim=1),\n",
    "            torch.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        )\n",
    "        # Hard loss\n",
    "        hard_loss = self.criterion_ce(student_logits, labels)\n",
    "        return self.alpha * soft_loss * (self.temperature ** 2) + (1.0 - self.alpha) * hard_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dabb6464-dba6-414d-99e5-af7d4bb6b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when all layers are trained\n",
    "def train_kd_res_all(teacher_model, student_model, train_loader, epochs=10, lr=1e-3, temperature=4.0, alpha=0.5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher_model.to(device).eval()\n",
    "    student_model.to(device).train()\n",
    "\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "    distillation_loss_fn = DistillationLoss(temperature, alpha)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Teacher model predictions (no gradient needed)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(images).logits\n",
    "\n",
    "            # Student model predictions\n",
    "            student_logits = student_model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = distillation_loss_fn(student_logits, teacher_logits, labels)\n",
    "\n",
    "            # Update optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(student_logits, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * total_correct / total_samples\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0123d9-3ba1-4edc-b9f4-4927822ec6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#when all layers are trained\n",
    "def train_kd_net_all(teacher_model, student_model, train_loader, epochs=10, lr=1e-3, temperature=4.0, alpha=0.5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher_model.to(device).eval()\n",
    "    student_model.to(device).train()\n",
    "\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "    distillation_loss_fn = DistillationLoss(temperature, alpha)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Teacher model predictions (no gradient needed)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(images)\n",
    "\n",
    "            # Student model predictions\n",
    "            student_logits = student_model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = distillation_loss_fn(student_logits, teacher_logits, labels)\n",
    "\n",
    "            # Update optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(student_logits, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * total_correct / total_samples\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f468136a-4bca-430d-a089-c025d9dfe56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kd_res_classifier(teacher_model, student_model, train_loader, epochs=10, lr=1e-3, temperature=4.0, alpha=0.5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher_model.to(device).eval()\n",
    "    student_model.to(device).train()\n",
    "\n",
    "    # ================================\n",
    "    # Freeze all layers except the classifier head\n",
    "    # ================================\n",
    "    for name, param in student_model.named_parameters():\n",
    "        if \"fc\" not in name and \"classifier\" not in name:  # Adjust for different models\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Use an optimizer that only updates trainable parameters\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, student_model.parameters()), lr=lr)\n",
    "    \n",
    "    distillation_loss_fn = DistillationLoss(temperature, alpha)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Teacher model predictions (no gradient needed)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(images).logits\n",
    "\n",
    "            # Student model predictions\n",
    "            student_logits = student_model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = distillation_loss_fn(student_logits, teacher_logits, labels)\n",
    "\n",
    "            # Update optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(student_logits, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * total_correct / total_samples\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe3d9b8-ac34-42b4-a12a-37ed641043e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kd_net_classifier(teacher_model, student_model, train_loader, epochs=10, lr=1e-3, temperature=4.0, alpha=0.5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher_model.to(device).eval()\n",
    "    student_model.to(device).train()\n",
    "\n",
    "    # ================================\n",
    "    # Freeze all layers except the classifier head\n",
    "    # ================================\n",
    "    for name, param in student_model.named_parameters():\n",
    "        if \"fc\" not in name and \"classifier\" not in name:  # Adjust for different models\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Use an optimizer that only updates trainable parameters\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, student_model.parameters()), lr=lr)\n",
    "    \n",
    "    distillation_loss_fn = DistillationLoss(temperature, alpha)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Teacher model predictions (no gradient needed)\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher_model(images)\n",
    "\n",
    "            # Student model predictions\n",
    "            student_logits = student_model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = distillation_loss_fn(student_logits, teacher_logits, labels)\n",
    "\n",
    "            # Update optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(student_logits, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * total_correct / total_samples\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0751c3f-2d5b-461a-a502-8962621bb5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device).eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe9bcd3-95bd-49ce-be57-058f6ebff104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, torch, torch.nn as nn\n",
    "\n",
    "def evaluate_model_performance_res(model, test_loader, baseline_accuracy=75.00, baseline_inference_time=0.10, baseline_model_size=100.0):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device).eval()\n",
    "    criterion, total, correct, running_loss, inference_times = nn.CrossEntropyLoss(), 0, 0, 0.0, []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            start_time = time.time(); outputs = model(images); end_time = time.time()\n",
    "            inference_times.append(end_time - start_time)\n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy, avg_loss, avg_inference_time = 100 * correct / total, running_loss / len(test_loader), sum(inference_times) / len(inference_times)\n",
    "    model_size = os.path.getsize(\"temp.pth\") / (1024 * 1024) if torch.save(model.state_dict(), \"temp.pth\") or os.path.exists(\"temp.pth\") else 0; os.remove(\"temp.pth\")\n",
    "    accuracy_drop, speed_improvement, size_reduction = baseline_accuracy - accuracy, (baseline_inference_time - avg_inference_time) / baseline_inference_time * 100, (baseline_model_size - model_size) / baseline_model_size * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}%, Average Loss: {avg_loss}, Model Size: {model_size} MB, Average Inference Time: {avg_inference_time}s\\nPerformance Comparison:\\nAccuracy Drop: {accuracy_drop}%, Speed Improvement: {speed_improvement}%, Size Reduction: {size_reduction}%\")\n",
    "    return accuracy, avg_loss, avg_inference_time, model_size\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model_performance_mobile(model, test_loader, baseline_accuracy=71.00, baseline_inference_time=0.05, baseline_model_size=20.0):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device).eval()\n",
    "    criterion, total, correct, running_loss, inference_times = nn.CrossEntropyLoss(), 0, 0, 0.0, []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            start_time = time.time(); outputs = model(images); end_time = time.time()\n",
    "            inference_times.append(end_time - start_time)\n",
    "            running_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy, avg_loss, avg_inference_time = 100 * correct / total, running_loss / len(test_loader), sum(inference_times) / len(inference_times)\n",
    "    model_size = os.path.getsize(\"temp.pth\") / (1024 * 1024) if torch.save(model.state_dict(), \"temp.pth\") or os.path.exists(\"temp.pth\") else 0; os.remove(\"temp.pth\")\n",
    "    accuracy_drop, speed_improvement, size_reduction = baseline_accuracy - accuracy, (baseline_inference_time - avg_inference_time) / baseline_inference_time * 100, (baseline_model_size - model_size) / baseline_model_size * 100\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}%, Average Loss: {avg_loss}, Model Size: {model_size} MB, Average Inference Time: {avg_inference_time}s\\nPerformance Comparison:\\nAccuracy Drop: {accuracy_drop}%, Speed Improvement: {speed_improvement}%, Size Reduction: {size_reduction}%\")\n",
    "    return accuracy, avg_loss, avg_inference_time, model_size\n",
    "\n",
    "def save_model(model, save_path):\n",
    "    \"\"\"Save the model's state dictionary to the specified path.\"\"\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22187546-f52a-47ed-8a63-42cdcff734b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 20:17:30.161088: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-17 20:17:32.010005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737134252.580133 3796820 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737134252.761396 3796820 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 20:17:34.362836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/saitaa0b/miniconda3/envs/CS294Y/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/saitaa0b/miniconda3/envs/CS294Y/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.8602, Accuracy: 62.84%\n",
      "Epoch [2/5], Loss: 0.8469, Accuracy: 80.40%\n",
      "Epoch [3/5], Loss: 0.6383, Accuracy: 85.92%\n",
      "Epoch [4/5], Loss: 0.5176, Accuracy: 89.77%\n",
      "Epoch [5/5], Loss: 0.4412, Accuracy: 92.25%\n",
      "Test Accuracy: 80.16%\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Teacher Model: Aznaur's ResNet-50\n",
    "# ====================================================\n",
    "model_name = \"jialicheng/cifar100-resnet-50\"\n",
    "teacher_model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# ====================================================\n",
    "# Student Model: ResNet-18\n",
    "# ====================================================\n",
    "student_model = models.resnet18(pretrained=True)\n",
    "student_model.fc = nn.Linear(student_model.fc.in_features, 100)  # Adjust for CIFAR-100\n",
    "student_model.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_kd_res_all(teacher_model, student_model, train_loader, epochs=5, lr=3e-4, temperature=4.0, alpha=0.5)\n",
    "    evaluate(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e427dfa8-ec6f-4eb3-94ae-02073d95c732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.09%, Average Loss: 0.6681001088287257, Model Size: 42.899356842041016 MB, Average Inference Time: 0.002709077883370315s\n",
      "Performance Comparison:\n",
      "Accuracy Drop: -5.090000000000003%, Speed Improvement: 97.29092211662969%, Size Reduction: 57.100643157958984%\n"
     ]
    }
   ],
   "source": [
    "accuracy, avg_loss, avg_inference_time, model_size = evaluate_model_performance_res(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58274ca4-8485-4777-91ad-e310bb1cacc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Student_Models/cifar100_resnet18.pth\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Student_Models/cifar100_resnet18.pth\" \n",
    "save_model(student_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff8a5f3f-b615-432d-ab0d-e2e223fdc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.4741, Accuracy: 57.79%\n",
      "Epoch [2/5], Loss: 0.9509, Accuracy: 80.96%\n",
      "Epoch [3/5], Loss: 0.8037, Accuracy: 88.11%\n",
      "Epoch [4/5], Loss: 0.7146, Accuracy: 92.43%\n",
      "Epoch [5/5], Loss: 0.6546, Accuracy: 95.14%\n",
      "Test Accuracy: 81.85%\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Teacher Model: EfficientNet B5 (from timm)\n",
    "# ====================================================\n",
    "teacher_model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=100)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ====================================================\n",
    "# Student Model: EfficientNet-Lite0 (from timm)\n",
    "# ====================================================\n",
    "student_model = timm.create_model('efficientnet_lite0', pretrained=True, num_classes=100)\n",
    "student_model.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_kd_net_all(teacher_model, student_model, train_loader, epochs=5, lr=3e-4, temperature=4.0, alpha=0.5)\n",
    "    evaluate(student_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "26de2b50-bb46-4f47-97ad-625f0b65f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.76%, Average Loss: 0.8752514659603939, Model Size: 13.591398239135742 MB, Average Inference Time: 0.005031636998623233s\n",
      "Performance Comparison:\n",
      "Accuracy Drop: -10.760000000000005%, Speed Improvement: 89.93672600275355%, Size Reduction: 32.04300880432129%\n"
     ]
    }
   ],
   "source": [
    "accuracy, avg_loss, avg_inference_time, model_size = evaluate_model_performance_net(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9cc8f201-330a-47ad-a201-37a7688d0875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Student_Models/cifar100_efficientnet_lite0.pth\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Student_Models/cifar100_efficientnet_lite0.pth\" \n",
    "save_model(student_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aaaef55f-a8a6-4a5d-9563-74678bc00870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.3664, Accuracy: 63.06%\n",
      "Epoch [2/5], Loss: 0.9022, Accuracy: 83.13%\n",
      "Epoch [3/5], Loss: 0.7662, Accuracy: 89.25%\n",
      "Epoch [4/5], Loss: 0.6856, Accuracy: 93.01%\n",
      "Epoch [5/5], Loss: 0.6274, Accuracy: 95.65%\n",
      "Test Accuracy: 82.79%\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Teacher Model: EfficientNet-B5 (from timm)\n",
    "# ====================================================\n",
    "teacher_model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=100)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ====================================================\n",
    "# Student Model: MobileNetV3-Large (from timm)\n",
    "# ====================================================\n",
    "student_model = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=100)\n",
    "student_model.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_kd_net_all(teacher_model, student_model, train_loader, epochs=5, lr=3e-4, temperature=4.0, alpha=0.5)\n",
    "    evaluate(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d04e19e-ea13-470e-ad11-d24351156f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.75%, Average Loss: 0.8692005895361116, Model Size: 16.70149040222168 MB, Average Inference Time: 0.006251126905030842s\n",
      "Performance Comparison:\n",
      "Accuracy Drop: -11.75%, Speed Improvement: 87.49774618993831%, Size Reduction: 16.4925479888916%\n"
     ]
    }
   ],
   "source": [
    "accuracy, avg_loss, avg_inference_time, model_size = evaluate_model_performance_mobile(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "81b3bdad-cc6b-468c-94d2-7478bdc8050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Student_Models/cifar100_mobilenetv3_large_100_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model after testing\n",
    "save_path = \"Student_Models/cifar100_mobilenetv3_large_100_model.pth\" \n",
    "save_model(student_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56bf0bc5-28da-4ab7-a690-255f3108e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.5223, Accuracy: 54.60%\n",
      "Epoch [2/5], Loss: 1.0790, Accuracy: 73.98%\n",
      "Epoch [3/5], Loss: 0.9515, Accuracy: 79.60%\n",
      "Epoch [4/5], Loss: 0.8751, Accuracy: 83.32%\n",
      "Epoch [5/5], Loss: 0.8178, Accuracy: 86.11%\n",
      "Test Accuracy: 72.97%\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Teacher Model: EfficientNet-B5 (from timm)\n",
    "# ====================================================\n",
    "teacher_model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=100)\n",
    "teacher_model.eval()\n",
    "\n",
    "# ====================================================\n",
    "# Student Model: MobileNetV3-Small (from timm)\n",
    "# ====================================================\n",
    "student_model = timm.create_model('mobilenetv3_small_100', pretrained=True, num_classes=100)\n",
    "student_model.train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_kd_net_all(teacher_model, student_model, train_loader, epochs=5, lr=3e-4, temperature=4.0, alpha=0.5)\n",
    "    evaluate(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ecc33f4-9b92-449c-9fc4-40c300bee103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.93%, Average Loss: 1.2232606161998798, Model Size: 6.297517776489258 MB, Average Inference Time: 0.005306020567688761s\n",
      "Performance Comparison:\n",
      "Accuracy Drop: -1.9300000000000068%, Speed Improvement: 89.38795886462249%, Size Reduction: 68.51241111755371%\n"
     ]
    }
   ],
   "source": [
    "accuracy, avg_loss, avg_inference_time, model_size = evaluate_model_performance_mobile(student_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "06ee1923-15c0-4257-883f-d3a36dd981f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Student_Models/cifar100_mobilenetv3_small_100.pth\n"
     ]
    }
   ],
   "source": [
    "save_path = \"Student_Models/cifar100_mobilenetv3_small_100.pth\" \n",
    "save_model(student_model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58161d6-8b0b-420f-a512-47551592faec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
